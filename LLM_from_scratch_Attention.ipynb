{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMuOKKNE0VIhZZZahLDArG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taaha3244/gpt2-scratch/blob/main/LLM_from_scratch_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NWRM8gBI6-W9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size, seq_len, d_in = x.shape\n",
        "\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, seq_len, d_out) @ (batch_size, d_out, seq_len)\n",
        "        # -> (batch_size, seq_len, seq_len)\n",
        "        attn_scores = queries @ keys.transpose(-2, -1)\n",
        "\n",
        "        # Scale attention scores\n",
        "        attn_scores = attn_scores / (self.d_out ** 0.5)\n",
        "\n",
        "        # Compute attention weights using softmax\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Compute context vectors\n",
        "        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, d_out)\n",
        "        # -> (batch_size, seq_len, d_out)\n",
        "        context_vectors = attn_weights @ values\n",
        "\n",
        "        return context_vectors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_len = 4\n",
        "d_in = 8\n",
        "d_out = 8\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_in)\n",
        "\n",
        "self_attention = SelfAttention(d_in=d_in, d_out=d_out)\n",
        "\n",
        "context_vectors = self_attention(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {context_vectors.shape}\")\n",
        "\n",
        "print(f\"Input shape: {x}\")\n",
        "print(f\"Output shape: {context_vectors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Rf8idJIM50",
        "outputId": "4229a373-deaf-47f9-b158-cf7836b2a33d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 8])\n",
            "Output shape: torch.Size([2, 4, 8])\n",
            "Input shape: tensor([[[ 0.2159, -0.3583, -0.4211, -0.7047,  0.3477,  2.0457, -0.2310,\n",
            "           0.8658],\n",
            "         [-0.1003,  0.2543, -1.0935, -0.5496,  0.5316,  0.5560,  0.6634,\n",
            "          -0.8767],\n",
            "         [-0.1346,  0.6268,  0.2724, -0.0927,  0.0892, -0.8980,  1.0593,\n",
            "          -0.7124],\n",
            "         [-0.3824, -0.2236, -0.9022,  0.3905,  1.2533,  0.5790,  1.1562,\n",
            "           0.5745]],\n",
            "\n",
            "        [[ 0.7469, -0.6548, -0.0366,  0.6984,  0.1737,  0.3693, -0.2300,\n",
            "           0.8148],\n",
            "         [-0.3705, -1.1015, -0.5517, -1.6435, -0.0266, -0.3467,  0.7775,\n",
            "          -1.2061],\n",
            "         [-0.7777,  1.3427, -1.1442, -0.3512,  0.4188, -0.7924, -0.2582,\n",
            "           1.2532],\n",
            "         [-0.3196,  0.7955, -0.5760,  0.0781,  0.0887,  1.1598,  0.9838,\n",
            "          -1.8984]]])\n",
            "Output shape: tensor([[[-0.3020,  0.1582, -0.1137, -0.1427, -0.1558,  0.2479,  0.0260,\n",
            "          -0.0947],\n",
            "         [-0.3074,  0.1372, -0.1072, -0.1462, -0.1590,  0.2403,  0.0393,\n",
            "          -0.0835],\n",
            "         [-0.2984,  0.1264, -0.1007, -0.1386, -0.1531,  0.2332,  0.0426,\n",
            "          -0.0743],\n",
            "         [-0.3091,  0.1493, -0.1077, -0.1474, -0.1630,  0.2449,  0.0312,\n",
            "          -0.0874]],\n",
            "\n",
            "        [[-0.2233,  0.0062, -0.0568, -0.1091, -0.1176,  0.1188,  0.0873,\n",
            "          -0.1511],\n",
            "         [-0.0150,  0.1761,  0.0941,  0.0242, -0.0912,  0.0500,  0.0071,\n",
            "          -0.1093],\n",
            "         [-0.2665, -0.0160, -0.1010, -0.1359, -0.1230,  0.1415,  0.0962,\n",
            "          -0.1669],\n",
            "         [-0.1848,  0.0103, -0.0017, -0.0861, -0.1130,  0.0884,  0.0888,\n",
            "          -0.1281]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout=0.1, qkv_bias=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Register causal mask buffer\n",
        "        # This creates a lower triangular matrix where future tokens are masked\n",
        "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size, seq_len, d_in = x.shape\n",
        "\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(-2, -1)\n",
        "\n",
        "\n",
        "        attn_scores = attn_scores / (self.d_out ** 0.5)\n",
        "\n",
        "        # Apply causal mask to prevent attention to future tokens\n",
        "        # We use the mask up to seq_len as input might be shorter than context_length\n",
        "        attn_scores = attn_scores.masked_fill(\n",
        "            self.mask[:seq_len, :seq_len],\n",
        "            float('-inf')\n",
        "        )\n",
        "\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vectors = attn_weights @ values\n",
        "\n",
        "        return context_vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "JVatZxge9m3b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads, dropout=0.1, qkv_bias=False):\n",
        "\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.W_out = nn.Linear(d_out, d_out)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size, seq_len, d_in = x.shape\n",
        "\n",
        "        # Project input to Query, Key, Value and split into heads\n",
        "        # Shape: (batch_size, seq_len, num_heads, head_dim)\n",
        "        queries = self.W_query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        keys = self.W_key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        values = self.W_value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        #(batch_size, num_heads, seq_len, head_dim)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, seq_len)\n",
        "        attn_scores = queries @ keys.transpose(-2, -1)\n",
        "\n",
        "        attn_scores = attn_scores / (self.head_dim ** 0.5)\n",
        "\n",
        "        attn_scores = attn_scores.masked_fill(\n",
        "            self.mask[:seq_len, :seq_len],\n",
        "            float('-inf')\n",
        "        )\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, head_dim)\n",
        "        context_vectors = attn_weights @ values\n",
        "\n",
        "        # Transpose and reshape to combine heads\n",
        "        # (batch_size, seq_len, d_out)\n",
        "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_out\n",
        "        )\n",
        "\n",
        "        # Apply output projection\n",
        "        context_vectors = self.W_out(context_vectors)\n",
        "\n",
        "        return context_vectors\n"
      ],
      "metadata": {
        "id": "50wDzN0qMXTw"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}