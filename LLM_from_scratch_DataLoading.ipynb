{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeJEQ7v3AD24eDvfO55mTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taaha3244/gpt2-scratch/blob/main/LLM_from_scratch_DataLoading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y1oVQYp2PkO",
        "outputId": "b465d46c-5fe4-4b66-e3e1-2e199a0f480b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4tsD_Bb18pn",
        "outputId": "fb3343d5-c399-47b5-af58-577fa5abf0c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu124\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "import re\n",
        "import tiktoken\n",
        "import torch\n",
        "import matplotlib\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_text(file_path: str) -> str:\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "      print(f\"Successfully loaded file with UTF-8 encoding\")\n",
        "\n",
        "    # To remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    print(f\"Total characters: {len(text)}\")\n",
        "    print(f\"Unique characters: {len(set(text))}\")\n",
        "    print(f\"\\nFirst 100 characters of cleaned text:\")\n",
        "    print(text[:101])\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_text = load_and_clean_text('/content/iqbal.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV_-U9iiZy0R",
        "outputId": "37afb20b-f8a8-426c-e39d-d43e271355fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded file with UTF-8 encoding\n",
            "Total characters: 349701\n",
            "Unique characters: 74\n",
            "\n",
            "First 100 characters of cleaned text:\n",
            "۱ گُلزارِ ہست و بود نہ بیگانہ وار دیکھ نہ آتے، ہمیں اس میں تکرار کیا تھی عجب و اعظ کی دِین داری ہے یا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UrduTokenizer:\n",
        "    def __init__(self):\n",
        "        # Special tokens with proper symbols\n",
        "        self.PAD_TOKEN = \"<PAD>\"\n",
        "        self.UNK_TOKEN = \"<UNK>\"\n",
        "        self.BOS_TOKEN = \"<BOS>\"\n",
        "        self.EOS_TOKEN = \"<EOS>\"\n",
        "\n",
        "        # Special token indices\n",
        "        self.PAD_IDX = 0\n",
        "        self.UNK_IDX = 1\n",
        "        self.BOS_IDX = 2\n",
        "        self.EOS_IDX = 3\n",
        "\n",
        "        # Initialize mappings\n",
        "        self.char_to_idx = {\n",
        "            self.PAD_TOKEN: self.PAD_IDX,\n",
        "            self.UNK_TOKEN: self.UNK_IDX,\n",
        "            self.BOS_TOKEN: self.BOS_IDX,\n",
        "            self.EOS_TOKEN: self.EOS_IDX\n",
        "        }\n",
        "        self.idx_to_char = {\n",
        "            self.PAD_IDX: self.PAD_TOKEN,\n",
        "            self.UNK_IDX: self.UNK_TOKEN,\n",
        "            self.BOS_IDX: self.BOS_TOKEN,\n",
        "            self.EOS_IDX: self.EOS_TOKEN\n",
        "        }\n",
        "        self.vocab_size = 4\n",
        "\n",
        "    def fit(self, text):\n",
        "        \"\"\"Build vocabulary from text\"\"\"\n",
        "        # Get unique characters from text\n",
        "        unique_chars = sorted(set(text))\n",
        "\n",
        "        # Add to vocabulary (after special tokens)\n",
        "        for char in unique_chars:\n",
        "            if char not in self.char_to_idx:\n",
        "                self.char_to_idx[char] = len(self.char_to_idx)\n",
        "                self.idx_to_char[len(self.idx_to_char)] = char\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        print(f\"Vocabulary statistics:\")\n",
        "        print(f\"Total vocab size: {self.vocab_size}\")\n",
        "        print(f\"Character vocab size: {len(unique_chars)}\")\n",
        "        print(f\"Special tokens: {[self.PAD_TOKEN, self.UNK_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN]}\")\n",
        "        return self\n",
        "\n",
        "    def encode(self, text, add_special_tokens=False):\n",
        "        \"\"\"Convert text to token ids\"\"\"\n",
        "        tokens = []\n",
        "\n",
        "        # Add BOS token\n",
        "        if add_special_tokens:\n",
        "            tokens.append(self.BOS_IDX)\n",
        "\n",
        "        # Encode characters\n",
        "        for char in text:\n",
        "            token_id = self.char_to_idx.get(char)\n",
        "            if token_id is None:\n",
        "                token_id = self.UNK_IDX\n",
        "            tokens.append(token_id)\n",
        "\n",
        "        # Add EOS token\n",
        "        if add_special_tokens:\n",
        "            tokens.append(self.EOS_IDX)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens, skip_special_tokens=True):\n",
        "        \"\"\"Convert token ids back to text\"\"\"\n",
        "        text = []\n",
        "        for token in tokens:\n",
        "            # Skip special tokens if requested\n",
        "            if skip_special_tokens and token in {self.PAD_IDX, self.BOS_IDX, self.EOS_IDX}:\n",
        "                continue\n",
        "\n",
        "            # Get character from idx_to_char mapping\n",
        "            char = self.idx_to_char.get(token)\n",
        "            if char in {self.PAD_TOKEN, self.UNK_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN}:\n",
        "                if not skip_special_tokens:\n",
        "                    text.append(char)\n",
        "            else:\n",
        "                text.append(char if char is not None else self.UNK_TOKEN)\n",
        "\n",
        "        return ''.join(text)\n",
        "\n",
        "    def pad_sequence(self, tokens, max_length):\n",
        "        \"\"\"Pad or truncate sequence to max_length\"\"\"\n",
        "        if len(tokens) > max_length:\n",
        "            return tokens[:max_length]\n",
        "        return tokens + [self.PAD_IDX] * (max_length - len(tokens))\n",
        "\n"
      ],
      "metadata": {
        "id": "edjSqsLsamaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokenizer(text):\n",
        "\n",
        "    tokenizer = UrduTokenizer()\n",
        "    tokenizer.fit(text)\n",
        "\n",
        "    sample = text[:50]\n",
        "    print(f\"\\nTesting with sample: {sample}\")\n",
        "\n",
        "\n",
        "    tokens = tokenizer.encode(sample)\n",
        "    decoded = tokenizer.decode(tokens)\n",
        "    print(f\"\\nEncoded tokens: {tokens[:10]}...\")\n",
        "    print(f\"Decoded text: {decoded}\")\n",
        "\n",
        "    tokens_special = tokenizer.encode(sample, add_special_tokens=True)\n",
        "    decoded_special = tokenizer.decode(tokens_special, skip_special_tokens=False)\n",
        "    print(f\"\\nWith special tokens:\")\n",
        "    print(f\"Encoded: {tokens_special[:10]}...\")\n",
        "    print(f\"Decoded: {decoded_special}\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = test_tokenizer(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIRXi1xQwFEL",
        "outputId": "7171393e-4dee-4bf4-8c59-9499ae3d5cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary statistics:\n",
            "Total vocab size: 78\n",
            "Character vocab size: 74\n",
            "Special tokens: ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
            "\n",
            "Testing with sample: ۱ گُلزارِ ہست و بود نہ بیگانہ وار دیکھ نہ آتے، ہمی\n",
            "\n",
            "Encoded tokens: [71, 4, 63, 47, 41, 30, 20, 29, 48, 4]...\n",
            "Decoded text: ۱ گُلزارِ ہست و بود نہ بیگانہ وار دیکھ نہ آتے، ہمی\n",
            "\n",
            "With special tokens:\n",
            "Encoded: [2, 71, 4, 63, 47, 41, 30, 20, 29, 48]...\n",
            "Decoded: <BOS>۱ گُلزارِ ہست و بود نہ بیگانہ وار دیکھ نہ آتے، ہمی<EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class UrduTextDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, max_length, stride):\n",
        "\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "\n",
        "            # Pad if necessary\n",
        "            input_chunk = tokenizer.pad_sequence(input_chunk, max_length)\n",
        "            target_chunk = tokenizer.pad_sequence(target_chunk, max_length)\n",
        "\n",
        "            # Convert to tensors\n",
        "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
        "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_data_loaders(text, tokenizer, max_length=128, stride=4,\n",
        "                       batch_size=4, val_split=0.1):\n",
        "\n",
        "\n",
        "    train_len = int(len(text) * (1 - val_split))\n",
        "    train_text = text[:train_len]\n",
        "    val_text = text[train_len:]\n",
        "\n",
        "    train_dataset = UrduTextDataset(\n",
        "        train_text, tokenizer, max_length, stride\n",
        "    )\n",
        "    val_dataset = UrduTextDataset(\n",
        "        val_text, tokenizer, max_length, stride\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataLoader Statistics:\")\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "GAajOMQgbRxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = create_data_loaders(\n",
        "    cleaned_text,\n",
        "    tokenizer,\n",
        "    max_length=128,\n",
        "    stride=8,\n",
        "    batch_size=8\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHQrRkxf-Kv",
        "outputId": "458b4dac-f423-496b-9e4d-83f7dce430d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataLoader Statistics:\n",
            "Training samples: 39326\n",
            "Validation samples: 4356\n",
            "Batch size: 8\n",
            "Training batches: 4916\n",
            "Validation batches: 545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNvRXQSwhiDk",
        "outputId": "0a13fc23-9547-4c05-f194-aeb3ccdb19e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[69, 64,  4,  ..., 24, 31,  4],\n",
              "         [62, 44, 19,  ...,  4, 62, 69],\n",
              "         [ 4, 20, 40,  ..., 69,  4, 31],\n",
              "         ...,\n",
              "         [29,  4, 40,  ..., 44, 69, 20],\n",
              "         [42, 24, 65,  ..., 48, 24, 20],\n",
              "         [ 4, 38, 42,  ..., 35, 20,  4]]),\n",
              " tensor([[64,  4, 57,  ..., 31,  4, 57],\n",
              "         [44, 19, 69,  ..., 62, 69,  4],\n",
              "         [20, 40, 21,  ...,  4, 31, 70],\n",
              "         ...,\n",
              "         [ 4, 40, 21,  ..., 69, 20,  4],\n",
              "         [24, 65,  4,  ..., 24, 20, 21],\n",
              "         [38, 42, 48,  ..., 20,  4, 67]])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}